### 华为云语音识别功能测试





### 科大讯飞语音听写：

##### 语音听写是可以实现说一个字识别一个字吗？还是说一句话说完再次识别？

>  目前语音听写不能实现一个字一个字识别的效果，单次识别是实时返回60秒以内的音频转文字。

#####  语音听写支持什么音频格式？

> 1.webapi听写普通版和sdk应用的语音听写：语音听写目前支持的格式是 pcm 和 wav 格式、音频采样率要是 16k 或者 8k（仅在线支持）、采样精度16 位、单声道音频；
> 2.webapi听写流式版：支持的格式是pcm、speex、speex-wb，音频采样率要是 16k 或者 8k、采样精度16 位、单声道音频，其中中文普通话和英文还支持mp3格式，可见[web流式听写开发文档 ](https://www.xfyun.cn/doc/asr/voicedictation/API.html#接口说明)。



### 华为云

| 接口类型         | 说明                                                         |
| ---------------- | ------------------------------------------------------------ |
| 实时语音识别接口 | 华为云提供的Websocket接口，主要用于实时语音识别。音频分片传输，服务器端可以返回中间临时转写结果，在最后返回最终转写结果。WebSocket接口调用； |
| 一句话识别       | 一句话识别接口，用于短语音（长度限制为一分钟）的同步识别。一次性上传整个音频，响应中即返回识别结果。REST API接口或者WebSocket接口调用 体验链接https://www.huaweicloud.com/ei/experiencespace/sis.html |
| 录音文件识别     | 录音文件识别接口，用于转写不超过5小时的音频。由于录音文件转写需要较长的时间，因此转写是异步的。 |

#### 实时语音识别

- 流式一句话

  WSS /v1/{project_id}/rasr/short-stream

- 实时语音识别连续模式

  WSS /v1/{project_id}/rasr/continue-stream

- 实时语音识别单句模式

  WSS /v1/{project_id}/rasr/sentence-stream



经过测试：实时语音识别模式 （ 连续模式和单句模式 ）其实都可以实时返回结果

##### 区别：

###### 流式一句话模式 

流式一句话模式的语音长度限制为一分钟，适合于对话聊天等识别场景。

该接口支持用户将一整段语音分段，以流式输入，最后得到识别结果。实时语音识别引擎在获得分段的输入语音的同时，就可以同步地对这段数据进行特征提取和解码工作，而不用等到所有数据都获得后再开始工作。因此这样就可以在最后一段语音结束后，仅延迟很短的时间（也即等待处理最后一段语音数据以及获取最终结果的时间）即可返回最终识别结果。这种流式输入方式能缩短整体上获得最终结果的时间，极大地提升用户体验。

###### 实时语音识别连续模式

连续识别模式的语音总长度限制为五小时，适合于会议、演讲和直播等场景。

连续识别模式在流式识别的基础上，结合了语音的端点检测功能。语音数据也是分段输入，但是连续识别模式将会在处理数据之前进行端点检测，如果是语音才会进行实际的解码工作，如果检测到静音，将直接丢弃。如果检测到一段语音的结束点，就会直接将当前这一段的识别结果返回，然后继续检测后面的语音数据。因此在连续识别模式中，可能多次返回识别结果。如果送入的一段语音较长，甚至有可能在一次返回中包括了多段的识别结果。

由于引入了静音检测，连续识别模式通常会比流式识别能具有更高的效率，因为对于静音段将不会进行特征提取和解码操作，因而能更有效地利用CPU。而流式识别通常和客户端的端点检测功能相结合，只将检测到的有效语音段上传到服务器进行识别。

###### 实时语音识别单句模式

单句模式自动检测一句话的结束，因此适合于需要与您的系统进行交互的场景，例如外呼、控制口令等场景。

实时语音识别引擎的单句识别模式，和连续识别模式类似，也会进行语音的端点检测，如果检测到静音，将直接丢弃，检测到语音才会馈入核心进行实际的解码工作，如果检测到一段语音的结束点，就会将当前这一段的识别结果返回。和连续识别不同的是，在单句模式下，返回第一段的识别结果后，将不再继续识别后续的音频。这主要是用于和用户进行语音交互的场景下，当用户说完一句话后，往往会等待后续的交互操作，例如聆听根据识别结果播报的相关内容，因而没有必要继续识别后续的音频。



##### 结论：

针对语音识别货源搜索的场景，实时语音识别单句模式更适用。



问题：

需要自行封装API， 转发麦克风录音内容

识别完成不会自动结束，需要在相应回调方法里添加结束代码

Android： 检测到句子结束事件 `public void onVoiceEnd()`

iOS:  没有相应的回调 但可以通过数据回调方法来看resp_type

`- (void)rasrClientReceiveMessage:(id)data;`

![截屏2022-10-11 17.44.50](/Users/admin/Library/Application Support/typora-user-images/截屏2022-10-11 17.44.50.png)

但其实iOS文档并没有标识出来，如果是单句模式，这里状态应该包括start， end， event， fatal_error四种

event里可以监听到一句话是否结束

**{"resp_type":"EVENT","trace_id":"82ec29df-333c-4d1b-8236-998b27250670","event":"VOICE_START","timestamp":1665481570}**

**{"resp_type":"EVENT","trace_id":"82ec29df-333c-4d1b-8236-998b27250670","event":"VOICE_END","timestamp":1665481570}**

整体暴露出的问题就是iOS方面API接口封装很简陋，拿到信息也只是把最原始的data json字符串打印出来，需要自行去一层一层判断分析拿到最终的text





实时语音识别服务，用户通过实时访问和调用API获取实时语音识别结果，支持的语言包含中文普通话、方言，方言当前支持**四川话、粤语和上海话**。

- 文本时间戳

  为音频转换结果生成特定的时间戳，从而通过搜索文本即可快速找到对应的原始音频。

- 智能断句

  通过提取上下文相关语义特征，并结合语音特征，智能划分断句及添加标点符号，提升输出文本的可阅读性。

- 中英文混合识别

  支持在中文句子识别中夹带英文字母、数字等，从而实现中、英文以及数字的混合识别。

- 即时输出识别结果

  连续识别语音流内容，即时输出结果，并可根据上下文语言模型自动校正。

- 自动静音检测

  对输入语音流进行静音检测，识别效率和准确率更高。

**产品优势**

- 识别准确率高

  采用最新一代语音识别技术，基于深度神经网络（Deep Neural Networks，简称DNN）技术，大大提高了抗噪性能，使识别准确率显著提升。

- 识别速度快

  把语言模型、词典和声学模型统一集成为一个大的神经网络，同时在工程上进行了大量的优化，大幅提升解码速度，使识别速度在业内处于领先地位。

- 多种识别模式

  支持多种实时语音识别模式，如流式识别、连续识别和实时识别模式，灵活适应不同应用场景。

- 定制化服务

  可定制特定垂直领域的语言层模型，可识别更多专有词汇和行业术语，进一步提高识别准确率。





### 验证实践

```objc
AK: UM2B3QI8ECPFY8NLG4SK

SK: kqMmzd9ipnlnJOKVzJD9IxI3b4g3S6zc4NqFWp0E

project_id:  84ca396357674a61aed21f0ff0d550fe

endpoint: sis-ext.cn-east-3.myhuaweicloud.com   
##华东-上海一  cn-east-3
```



科大讯飞准确率更高 

飞翼车

厢式 

发音准确问题都不大，有一些同音词，







### 一些问题：

1. 无法设置**麦克风**录入作为audio source需要手动转发

2. Sdk使用文档和demo不够充实，API简易

   android提供了demo

   iOS没有demo只有一段简单的示例代码，需要自行补充如何实现麦克风录音以及连续发送语音识别的逻辑，如需使用实时语音识别可能需要客户端提供一些封装

3. 报错信息不够明确 （连接失败报错，账户冻结报错）

```objc
iOS：{"resp_type":"ERROR","trace_id":"9a1da4eb-a728-45fc-9c88-77aefd9ad802","error_code":"SIS.0032","error_msg":"need to send start command to start"}
//其实要翻到很上面才会发现有账户冻结报错
//如果账户冻结问题应该终止后续步骤，直接报错即可，比较简明
```

4. 相对比科大讯飞的语音识别功能， 需要自行判定语音是否结束，android demo提供了结束选项



方向：

1. 单句模式是否必要？是否其他模式可以自识别一句话结束？

android一句话模式？ websocket

iOS 单句模式？ websocket

2. iOS转发功能参考android实现